---
title: "Platforms and Power"
subtitle: "Visualising the changing length of user agreements"
author: "James Goldie, 360info"
date: "2022-05-09"
code-fold: true
theme: style/article.scss
---

This demo analyses the changing length of the user agreements that major tech companies use. We'll use the [Wayback Machine API](https://archive.org/help/wayback_api.php) to identify snapshots of platform APIs, extract the text at different points in time, and analyse their word length.


```{r}
library(tidyverse)
library(httr)
library(rvest)
library(stringr)
library(tidytext)
library(lubridate)
library(collateral)
library(themes360info)
library(here)
```

## Test case: Facebook

Facebook's Terms of Service are currently at <https://www.facebook.com/terms.php>. Let's see the most recent Wayback Machine snapshot:

```{r}
#| label: demolookup
paste0(
  "http://archive.org/wayback/available?url=",
  "https://www.facebook.com/terms.php") %>%
  GET() ->
fb_snapshot

content(fb_snapshot)
```

We can access the newest snapshot at <`r content(fb_snapshot)$archived_snapshots$closest$url`> using the `{rvest}` package. The text of the agreement (as well as buttons to jump to the various sections) is available in the `#content` node:

```{r}
#| label: scrapefn
#| code-fold: true
#| code-summary: "Show the scraping and extraction function"

#' @param url The url of the page to look up (eg.
#'   https://www.facebook.com/terms.php)
#' @param css The css selector to use to extract text
#' @param dt A Date object, or a string in YYMMDD format, to try and get the
#'   snapshot from. If ommitted, the latest is used.
#' @return A list containing:
#'   - dt: the actual date-time of the snapshot
#'   - url: the url of the snapshot
#'   - words: a tidy data frame of word tokens as from tidytext::unnest_tokens()
get_words <- function(url, css, dt = NULL) {

  # construct the url to lookup the snapshot (add timestamp if requested)
  lookup_url <- paste0("http://archive.org/wayback/available?url=", url)
  if (!is_empty(dt)) {
    if (class(dt) == "Date") {
      lookup_url <- paste0(lookup_url, "&timestamp=", format(dt, "%Y%M%d"))
    } else {
      lookup_url <- paste0(lookup_url, "&timestamp=", dt)
    }
  }

  response <- GET(lookup_url)

  # check for missing content
  if (response$status != 200L) {
    warning(paste("HTTP", response$status, "thrown"))
  }
  is_available <- content(response)$archived_snapshots$closest$available
  if (is_empty(is_available) || (!is_available)) {
    stop("No snapshot available")
  }

  # extract the actual snapshot time and url
  snapshot_url <- content(response)$archived_snapshots$closest$url
  snapshot_dt <-
    content(response)$archived_snapshots$closest$timestamp %>%
    ymd_hms()

  # extract the snapshot content
  scrape <- read_html(snapshot_url)

  # extract the text from the page and break it by paragraph
  scrape %>%
    html_element(css) %>%
    html_text2() %>%
    str_split(regex("\n+")) %>%
    pluck(1) ->
  scrape_text

  scrape_text %>%
    tibble(line = 1:length(.), text = .) %>%
    unnest_tokens(word, text) ->
  scrape_words

  # return the words and the 
  return(list(
    dt = snapshot_dt,
    lookup_url = lookup_url,
    snapshot_url = snapshot_url,
    words = scrape_words
  ))
}
```

```{r}
demo_words <- get_words("https://www.facebook.com/terms.php", "#content")
```

The terms (including the Meta transition preface and the buttons) contain `r nrow(demo_words)` words.

### Comparing to past snapshots

But we want to see what this was like in the past. The Wayback Machine has thousands of scrapes of this URL over the last 15 years, and I don't think we can look at all. But we could probably look at, say, one each month. The real question is whether the page structure changes significantly enough over time to throw our function off.

[This scrape from 2018](https://web.archive.org/web/20180404004922/https://www.facebook.com/terms.php) still has the content in `#content`, which is good. So does [this scrape from 2015](https://web.archive.org/web/20150312205022/https://www.facebook.com/terms.php). [This one in 2012](https://web.archive.org/web/20120214045718/https://www.facebook.com/terms.php) redirects to `/legal/terms` (but scans fine). Same ad [this one in 2008](https://web.archive.org/web/20080220201805/https://www.facebook.com/terms.php).

Sampling a few scrapes quickly, it looks like the page structure has been consistent all the way back to the first Wayback snapshots back in [late 2005](https://web.archive.org/web/20051228025855/https://www.facebook.com/terms.php). So we're looking pretty good here!

:::{.callout-note}
Most of these terms link out to "supplementary terms of use" that govern the use of optional features or related products. I'm not going to try to analyse these for now, but we might come back to them!
:::

```{r}
seq(as.Date("2005-12-15"), Sys.Date(), by = "month") %>%
  tibble(
    url = "https://www.facebook.com/terms.php",
    css = "#content",
    dt = .) %>%
  # test: first and last two rows
  # slice(c(1:2, (n() - 1):n())) %>%
  mutate(
    words = pmap_peacefully(., get_words),
    lookup_url = map_chr(words, c("result", "lookup_url")),
    snapshot_url = map_chr(words, c("result", "snapshot_url")),
    par_count = map_int(words, ~ max(.x$result$words$line)),
    word_count = map_int(words, ~ nrow(.x$result$words))) ->
fb_terms_primary
```

How does that look?

```{r}
fb_terms_primary %>%
{
  ggplot(.) +
    aes(x = dt, y = word_count) +
    geom_area(fill = alpha("#3b5998", 0.75), colour = "#3b5998") +
    theme_360() +
    theme(
      panel.grid.major.x = element_blank(),
      panel.grid.minor.x = element_blank()
    ) +
    labs(x = NULL, y = "Word count",
      title = toupper("Facebook terms of use"))
}
```