---
title: "Platforms and Power"
subtitle: "Visualising the changing length of user agreements"
author: "James Goldie, 360info"
date: "2022-05-09"
code-fold: true
theme: style/article.scss
---

This demo analyses the changing length of the user agreements that major tech companies use. We'll use the [Wayback Machine API](https://archive.org/help/wayback_api.php) to identify snapshots of platform APIs, extract the text at different points in time, and analyse their word length.


```{r}
library(tidyverse)
library(httr)
library(rvest)
library(stringr)
library(tidytext)
library(lubridate)
library(collateral)
library(themes360info)
library(here)
```

## Test case: Facebook

Facebook's Terms of Service are currently at <https://www.facebook.com/terms.php>. Let's see the most recent Wayback Machine snapshot:

```{r}
#| label: demolookup
paste0(
  "http://archive.org/wayback/available?url=",
  "https://www.facebook.com/terms.php") %>%
  GET() ->
fb_snapshot

content(fb_snapshot)
```

We can access the newest snapshot at <`r content(fb_snapshot)$archived_snapshots$closest$url`> using the `{rvest}` package. The text of the agreement (as well as buttons to jump to the various sections) is available in the `#content` node:

```{r}
#| label: scrapefn
#| code-fold: true
#| code-summary: "Show the scraping and extraction function"

#' @param url The url of the page to look up (eg.
#'   https://www.facebook.com/terms.php)
#' @param css The css selector to use to extract text
#' @param dt A Date object, or a string in YYMMDD format, to try and get the
#'   snapshot from. If ommitted, the latest is used.
#' @return A list containing:
#'   - dt: the actual date-time of the snapshot
#'   - url: the url of the snapshot
#'   - words: a tidy data frame of word tokens as from tidytext::unnest_tokens()
get_words <- function(url, css, dt = NULL) {

  # construct the url to lookup the snapshot (add timestamp if requested)
  lookup_url <- paste0("http://archive.org/wayback/available?url=", url)
  if (!is_empty(dt)) {
    if (class(dt) == "Date") {
      lookup_url <- paste0(lookup_url, "&timestamp=", format(dt, "%Y%M%d"))
    } else {
      lookup_url <- paste0(lookup_url, "&timestamp=", dt)
    }
  }

  response <- GET(lookup_url)

  # check for missing content
  if (response$status != 200L) {
    warning(paste("HTTP", response$status, "thrown"))
  }
  is_available <- content(response)$archived_snapshots$closest$available
  if (is_empty(is_available) || (!is_available)) {
    stop("No snapshot available")
  }

  # extract the actual snapshot time and url
  snapshot_url <- content(response)$archived_snapshots$closest$url
  snapshot_dt <-
    content(response)$archived_snapshots$closest$timestamp %>%
    ymd_hms()


  # extract the snapshot content
  scrape <- read_html(snapshot_url)

  # extract the text from the page and break it by paragraph
  scrape %>%
    html_element(css) %>%
    html_text2() %>%
    str_split(regex("\n+")) %>%
    pluck(1) ->
  scrape_text

  scrape_text %>%
    tibble(line = 1:length(.), text = .) %>%
    unnest_tokens(word, text) ->
  scrape_words

  # return the words and the 
  return(list(
    dt = snapshot_dt,
    lookup_url = lookup_url,
    snapshot_url = snapshot_url,
    words = scrape_words
  ))
}
```

```{r}
demo_words <- get_words("https://www.facebook.com/terms.php", "#content")
```

The terms (including the Meta transition preface and the buttons) contain `r nrow(demo_words)` words.

### Comparing to past snapshots

But we want to see what this was like in the past. The Wayback Machine has thousands of scrapes of this URL over the last 15 years, and I don't think we can look at all. But we could probably look at, say, one each month. The real question is whether the page structure changes significantly enough over time to throw our function off.

[This scrape from 2018](https://web.archive.org/web/20180404004922/https://www.facebook.com/terms.php) still has the content in `#content`, which is good. So does [this scrape from 2015](https://web.archive.org/web/20150312205022/https://www.facebook.com/terms.php). [This one in 2012](https://web.archive.org/web/20120214045718/https://www.facebook.com/terms.php) redirects to `/legal/terms` (but scans fine). Same for [this one in 2008](https://web.archive.org/web/20080220201805/https://www.facebook.com/terms.php).

Sampling a few scrapes quickly, it looks like the page structure has been consistent all the way back to the first Wayback snapshots back in [late 2005](https://web.archive.org/web/20051228025855/https://www.facebook.com/terms.php). So we're looking pretty good here!

:::{.callout-note}
Most of these terms link out to "supplementary terms of use" that govern the use of optional features or related products. I'm not going to try to analyse these for now, but we might come back to them!
:::

```{r}
seq(as.Date("2005-12-15"), Sys.Date(), by = "month") %>%
  tibble(
    url = "https://www.facebook.com/terms.php",
    css = "#content",
    dt = .) %>%
  # test: first and last two rows
  # slice(c(1:2, (n() - 1):n())) %>%
  mutate(
    words = pmap_peacefully(., get_words),
    lookup_url = map_chr(words, c("result", "lookup_url")),
    snapshot_url = map_chr(words, c("result", "snapshot_url")),
    par_count = map_int(words, ~ max(.x$result$words$line)),
    word_count = map_int(words, ~ nrow(.x$result$words))) ->
fb_terms_primary
```

How does that look?

```{r}
fb_terms_primary %>%
{
  ggplot(.) +
    aes(x = dt, y = word_count) +
    geom_area(fill = alpha("#3b5998", 0.75), colour = "#3b5998") +
    theme_360() +
    theme(
      panel.grid.major.x = element_blank(),
      panel.grid.minor.x = element_blank()
    ) +
    labs(x = NULL, y = "Word count",
      title = toupper("Facebook terms of use"))
}
```

I'm interested in why the terms spiked so high in the late 2000s before dropping back down in 2010. My suspicion is that Facebook has broken its terms up into "supplementary" terms of use for parts of its products, like its [Facebook Pages, Groups and Events Policy](Facebook Pages, Groups and Events Policy). Most people would consider groups and events fairly core features of Facebook, but these terms are broken into a separate document.

### Getting supplementary terms

One approach to handling these terms is to modify our scraper to pull out links and scrape them for the same date too. We need to be careful to screen out duplicate links too, though, and if those terms in turn link to others, we'd need to screen out ones we'd already scraped to avoid circular definitions.

Let's start with the snapshot request function:

```{r}
#| label: reqsnapshotfn
#| code-fold: true
#| code-summary: "Show the snapshot request function"

#' @param url The url of the page to look up (eg.
#'   https://www.facebook.com/terms.php)
#' @param dt A Date object, or a string in YYMMDD format, to try and get the
#'   snapshot from. If ommitted, the latest is used.
#' @return A list containing:
#'   - dt: the actual date-time of the snapshot
#'   - snapshot_url: the url of the snapshot
request_snapshot <- function(url, dt) {

  # construct the url to lookup the snapshot (add timestamp if requested)
  lookup_url <- paste0("http://archive.org/wayback/available?url=", url)
  if (!is_empty(dt)) {
    if (class(dt) == "Date") {
      lookup_url <- paste0(lookup_url, "&timestamp=", format(dt, "%Y%M%d"))
    } else {
      lookup_url <- paste0(lookup_url, "&timestamp=", dt)
    }
  }

  response <- GET(lookup_url)

  # check for missing content
  if (response$status != 200L) {
    warning(paste("HTTP", response$status, "thrown"))
  }
  is_available <- content(response)$archived_snapshots$closest$available
  if (is_empty(is_available) || (!is_available)) {
    stop("No snapshot available")
  }

  # extract the actual snapshot time and url
  snapshot_url <- content(response)$archived_snapshots$closest$url
  snapshot_dt <-
    content(response)$archived_snapshots$closest$timestamp %>%
    ymd_hms()

  return(list(
    snapshot_dt = snapshot_dt,
    snapshot_url = snapshot_url))
}
```

And now the scraping function. One challenge here is that Facebook's policy pages have adopted many designs over the years (and even at the same time for different policies), and so the CSS selector required varies. I've adopted a fallback approach, where the function takes a _vector_ of potential CSS selectors. It tries them in succession until it finds one in the document and then uses that to extract the text:

```{r}
#| label: getwordslinksfn
#| code-fold: true
#| code-summary: "Show the words and links scraping function"

#' @param url The url of the page snapshot to look up (eg.
#'   https://web.archive.org/web/20220223013629/
#'     https://www.facebook.com/terms.php)
#' @param css A vector of CSS selectors from which to attempt to extract article
#'   text. These are tried successively until one is found in the page.
#' @return A list containing:
#'   - dt: the actual date-time of the snapshot
#'   - url: the url of the snapshot
#'   - words: a tidy data frame of word tokens as from tidytext::unnest_tokens
get_words_and_links <- function(snapshot_url, css) {

  # extract the snapshot content
  scrape <- read_html(snapshot_url)

  # first, let's try to detect the selector that has the page content in it. this
  # varies by page and over time!
  # css_tries <- c("section._9lea", "#rebrandBodyID", "#content")
  for (css_try in css) {
    scrape_content <- scrape %>% html_element(css_try)
    if (length(scrape_content) > 0L) {
      break;
    }
  }
  if (length(scrape_content) == 0L) {
    stop("Couldn't auto-detect article content using supplied CSS")
  }

  # identify links
  scrape_content %>%
    html_elements("a") %>%
    { tibble(url = html_attr(., "href"), label = html_text(.)) } %>%
    # <a> that contain "policy", "policies", "terms"... but not "printable")
    filter(str_detect(label,
      regex("terms|policy|policies|notice", ignore_case = TRUE))) %>%
    filter(str_detect(label, coll("printable", ignore_case = TRUE),
      negate = TRUE)) %>%
    filter(str_detect(label, coll("plain text", ignore_case = TRUE),
      negate = TRUE)) %>%
    distinct(url, .keep_all = TRUE) %>%
    mutate(label = str_trim(label)) ->
  scrape_links

  # extract the text from the page and break it by paragraph
  scrape_content %>%
    html_text2() %>%
    str_split(regex("\n+")) %>%
    pluck(1) ->
  scrape_text

  scrape_text %>%
    tibble(line = 1:length(.), text = .) %>%
    unnest_tokens(word, text) ->
  scrape_words

  # return the words and the 
  return(list(
    words = scrape_words,
    links = scrape_links
  ))
}
```

Now we can re-run this scraper - first on the primary terms, then on the extracted links! Although Waybaack Machine does substitute links in snapshotted terms with snapshotted URLs, I've opted not to use them and to re-request a nearby snapshot from the original link URL, as related policies are not always snapshotted at the exact same time.

```{r}
#| label: fbcss
fb_css_tries <- c("section._9lea", "#rebrandBodyID", "#content",
  ".documentation .content", "body table.bordertable[border=\"1\"]",
  "div[role=\"main\"]")
```

```{r}
#| label: fbprimaryscrape

fb_primary_terms <- tibble(
  url = "https://www.facebook.com/legal/terms",
  type = "primary")
dt <- seq(as.Date("2005-12-15"), Sys.Date(), by = "month")

expand_grid(fb_primary_terms, dt) %>%
  # test: first and last two rows
  group_by(url) %>%
  slice(c(1:2, (n() - 1):n())) %>%
  ungroup() %>%
  mutate(
    # do the intiial lookup
    lookup = map2_peacefully(url, dt, request_snapshot),
    prim_snapshot_dt = map(lookup, c("result", "snapshot_dt")),
    prim_snapshot_url = map_chr(lookup, c("result", "snapshot_url")),
    # then scrape the snapshot
    prim_scrape = map_peacefully(prim_snapshot_url, get_words_and_links,
      css = fb_css_tries),
    prim_words = map(prim_scrape, c("result", "words")),
    prim_links = map(prim_scrape, c("result", "links"))) %>%
  unnest(prim_snapshot_dt) ->
fb_terms_firststage
```

Let's find out what the secondary policies are:

```{r}
fb_terms_firststage %>%
  select(dt, prim_links) %>%
  unnest_longer(prim_links) %>%
  unpack(prim_links) %>%
  # discard NAs and duplicate policies
  distinct(dt, url, .keep_all = TRUE) %>%
  mutate(
    original_url = str_replace(
      str_replace(url, fixed("http://web.archive.org"), ""),
      regex("/web/[:digit:]{14}/"), ""),
    # do the initial lookup
    lookup = map2_peacefully(original_url, dt, request_snapshot),
    sec_snapshot_dt = map(lookup, c("result", "snapshot_dt"), .null = NA),
    sec_snapshot_url = map_chr(lookup, c("result", "snapshot_url"), .null = NA),
    sec_scrape = map_peacefully(sec_snapshot_url, get_words_and_links,
      css = fb_css_tries),
    sec_words = map(sec_scrape, c("result", "words"))) %>%
  unnest(sec_snapshot_dt) %>%
  mutate(type = "secondary") ->
fb_terms_secondary
```
