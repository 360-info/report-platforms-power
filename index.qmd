---
title: "Platforms and Power"
subtitle: "Visualising the changing length of user agreements"
author: "James Goldie, 360info"
date: "2022-05-09"
code-fold: true
theme: style/article.scss
---

This demo analyses the changing length of the user agreements that major tech companies use. We'll use the [Wayback Machine API](https://archive.org/help/wayback_api.php) to identify snapshots of platform APIs, extract the text at different points in time, and analyse their word length.


```{r}
library(tidyverse)
library(urltools)
library(httr)
library(rvest)
library(stringr)
library(tidytext)
library(lubridate)
library(collateral)
library(themes360info)
library(here)
```

## Introduction

The Wayback Machine has thousands of scrapes of [Facebook's Terms of Service](https://www.facebook.com/terms.php) over the last 15 years, and I don't think we can look at all of them. But we could probably look at, say, one each month.

One thing I've noticed is that Facebook's Terms link out to a bunch of other policies and agreements, governing various feratures. Some of them, like Developer Payment Policies, may only affect a small subset of users - but others, like Groups, Pages and Events, would cover most regular Facebook users!

One approach to handling these ancillary agreements is to modify our scraper to pull out links from the primary agreement and to scrape those for the same date too.

We need to be careful to screen out duplicate links too, though, and if those terms in turn link to others, we'd need to screen out ones we'd already scraped to avoid circular definitions. For now, let's just recurse one level, rather than trying to map out an entire tree of agreements.

Let's start with the snapshot request function:

```{r}
#| label: reqsnapshotfn
#| code-fold: true
#| code-summary: "Show the snapshot request function"

#' @param url The url of the page to look up (eg.
#'   https://www.facebook.com/terms.php)
#' @param dt A Date object, or a string in YYMMDD format, to try and get the
#'   snapshot from. If ommitted, the latest is used.
#' @return A list containing:
#'   - dt: the actual date-time of the snapshot
#'   - snapshot_url: the url of the snapshot
request_snapshot <- function(url, dt) {

  # construct the url to lookup the snapshot (add timestamp if requested)
  lookup_url <- paste0("http://archive.org/wayback/available?url=", url)
  if (!is_empty(dt)) {
    if (class(dt) == "Date") {
      lookup_url <- paste0(lookup_url, "&timestamp=", format(dt, "%Y%M%d"))
    } else {
      lookup_url <- paste0(lookup_url, "&timestamp=", dt)
    }
  }

  response <- GET(lookup_url)

  # check for missing content
  if (response$status != 200L) {
    warning(paste("HTTP", response$status, "thrown"))
  }
  is_available <- content(response)$archived_snapshots$closest$available
  if (is_empty(is_available) || (!is_available)) {
    stop("No snapshot available")
  }

  # extract the actual snapshot time and url
  snapshot_url <- content(response)$archived_snapshots$closest$url
  snapshot_dt <-
    content(response)$archived_snapshots$closest$timestamp %>%
    ymd_hms()

  return(list(
    snapshot_dt = snapshot_dt,
    snapshot_url = snapshot_url))
}
```

And now the scraping function. One challenge here is that Facebook's policy pages have adopted many designs over the years (and even at the same time for different policies), and so the CSS selector required varies. I've adopted a fallback approach, where the function takes a _vector_ of potential CSS selectors. It tries them in succession until it finds one in the document and then uses that to extract the text:

```{r}
#| label: getwordslinksfn
#| code-fold: true
#| code-summary: "Show the words and links scraping function"

#' @param url The url of the page snapshot to look up (eg.
#'   https://web.archive.org/web/20220223013629/
#'     https://www.facebook.com/terms.php)
#' @param css A vector of CSS selectors from which to attempt to extract article
#'   text. These are tried successively until one is found in the page.
#' @return A list containing:
#'   - dt: the actual date-time of the snapshot
#'   - url: the url of the snapshot
#'   - words: a tidy data frame of word tokens by paragraph, as returned by
#'     tidytext::unnest_tokens
get_words_and_links <- function(snapshot_url, css) {

  # extract the snapshot content
  scrape <- read_html(snapshot_url)

  # first, let's try to detect the selector that has the page content in it. this
  # varies by page and over time!
  # css_tries <- c("section._9lea", "#rebrandBodyID", "#content")
  for (css_try in css) {
    scrape_content <- scrape %>% html_element(css_try)
    if (length(scrape_content) > 0L) {
      break;
    }
  }
  if (length(scrape_content) == 0L) {
    stop("Couldn't auto-detect article content using supplied CSS")
  }

  # identify links
  scrape_content %>%
    html_elements("a") %>%
    { tibble(url = html_attr(., "href"), label = html_text(.)) } %>%
    # <a> based on label
    filter(str_detect(label,
      regex("terms|policy|policies|notice|procedure|guideline|tips|here",
        ignore_case = TRUE))) %>%
    filter(str_detect(label, coll("printable", ignore_case = TRUE),
      negate = TRUE)) %>%
    filter(str_detect(label, coll("plain text", ignore_case = TRUE),
      negate = TRUE)) %>%
    filter(str_detect(label, coll("contact", ignore_case = TRUE),
      negate = TRUE)) %>%
    filter(str_detect(label, coll("support", ignore_case = TRUE),
      negate = TRUE)) ->
  scrape_links_untidy
  
  # remove the fragments from the urls (so that we can detect and remove
  # internal links, like tables of contents)
  fragment(scrape_links_untidy$url) <- NULL
  
  # now remove duplicate links (and internal ones)
  # and clean up white space in labels
  scrape_links_untidy %>%
    distinct(url, .keep_all = TRUE) %>%
    filter(url != snapshot_url) %>%
    mutate(label = str_trim(label)) ->
  scrape_links
  
  # extract the text from the page and break it by paragraph
  scrape_content %>%
    html_text2() %>%
    str_split(regex("\n+")) %>%
    pluck(1) ->
  scrape_text

  # finally, unnest the words of the agreement
  scrape_text %>%
    tibble(para = 1:length(.), text = .) %>%
    unnest_tokens(word, text) ->
  scrape_words

  # return the words and the 
  return(list(
    words = scrape_words,
    links = scrape_links
  ))
}
```

A quick function to save our processed word counts:

```{r}
#| label: savetermsfn
#| code-fold: true
#| code-summary: "Show the term saving function"

save_terms <- function(terms, date, policy, platform) {
  dir.create(here("data", "terms", platform, date), recursive = TRUE)
  
  policy_safe <- str_to_lower(str_replace_all(policy, " ", "-"))
  write_csv(terms, here("data", "terms", platform, date,
    paste0(policy_safe, ".csv")))
}
```

Now, finally, let's encapsulate the higher level tidying:

```{r}
#| label: analysisfn

analyse_platform <- function(platform, primary_url, start_date, css_tries,
  test = FALSE) {
  
  message("Beginning analysis...")

  # assemble the combos of link requests over time
  primary_terms <- tibble(url = primary_url, type = "primary")
  dt <- seq(start_date, Sys.Date(), by = "month")
  primary_term_history <- expand_grid(primary_terms, dt)

  # if testing, jus ttake the first and last 2 dates for each url
  if (test) {
    message("TEST MODE ON")
    primary_term_history <-
      primary_term_history %>%
      group_by(url) %>%
      slice(c(1:2, (n() - 1):n())) %>%
      ungroup()
  }

  # --- first round: scrape the primary terms of use --------------------------

  message("Scraping primary terms of use...")

  primary_term_history %>%
    mutate(
      # do the intiial lookup
      lookup = map2_peacefully(url, dt, request_snapshot),
      prim_snapshot_dt = map(lookup, c("result", "snapshot_dt")),
      prim_snapshot_url = map_chr(lookup, c("result", "snapshot_url")),
      # then scrape the snapshot
      prim_scrape = map_peacefully(prim_snapshot_url, get_words_and_links,
        css = css_tries),
      prim_words = map(prim_scrape, c("result", "words")),
      prim_links = map(prim_scrape, c("result", "links"))) %>%
    unnest(prim_snapshot_dt) %>%
    mutate(
      label = "Terms of Use",
      word_count = map_int(prim_words,
        ~ ifelse(!is_empty(.x), nrow(.x), NA_integer_))) ->
  terms_firststage

  message("Writing primary terms of use to disk...")

  # write the processed terms out to csvs
  terms_firststage %>%
    select(terms = prim_words, date = dt, policy = label) %>%
    pwalk(save_terms, platform = platform)

  # --- second round: scrape the discovered links -----------------------------

  message("Scraping discovered links...")

  terms_firststage %>%
    select(dt, prim_links) %>%
    unnest_longer(prim_links) %>%
    unpack(prim_links) %>%
    # discard NAs and duplicate policies
    distinct(dt, url, .keep_all = TRUE) %>%
    drop_na(url) %>%
    mutate(
      # recover the original link from the wayback-substituted one
      original_url = str_replace(
        str_replace(url, fixed("http://web.archive.org"), ""),
        regex("/web/[:digit:]{14}/"), ""),
      # do the initial lookup
      lookup = map2_peacefully(original_url, dt, request_snapshot),
      sec_snapshot_dt = map(lookup, c("result", "snapshot_dt"), .null = NA),
      sec_snapshot_url = map_chr(lookup, c("result", "snapshot_url"), .null = NA),
      sec_scrape = map_peacefully(sec_snapshot_url, get_words_and_links,
        css = css_tries),
      sec_words = map(sec_scrape, c("result", "words"))) %>%
    unnest(sec_snapshot_dt) %>%
    mutate(
      type = "secondary",
      word_count = map_int(sec_words,
        ~ ifelse(!is_empty(.x), nrow(.x), NA_integer_))) ->
  terms_secondary

  message("Writing secondary agreements to disk...")

  # write the processed terms out to csvs
  terms_secondary %>%
    select(terms = sec_words, date = dt, policy = label) %>%
    pwalk(save_terms, platform = platform)

  # --- merge primary and secondary terms -------------------------------------

  message("Merging primary and secondary agreement word counts...")

  bind_rows(
    terms_firststage %>%
      select(type, label, url, dt,
        snapshot_dt = prim_snapshot_dt, snapshot_url = prim_snapshot_url,
        word_count),
    terms_secondary %>%
      select(type, label, url = original_url, dt,
        snapshot_dt = sec_snapshot_dt, snapshot_url = sec_snapshot_url,
        word_count)) %>%
    rename(policy_name = label, url = target_url, target_dt = dt) ->
  terms_all

  message("Done!")
  return(terms_all)
}
```

Now that we have our essential machinery, we can run this scraper - first on Facebook's primary terms of use over time, then on the extracted links!

## Facebook

```{r}
#| label: scrapefb
fb_words <- analyse_platform("facebook",
  primary_url = "https://www.facebook.com/terms.php",
  start_date = as.Date("2005-12-15"),
  css_tries = c("section._9lea", "#rebrandBodyID", "#content",
    ".documentation .content", "body table.bordertable[border=\"1\"]",
    "div[role=\"main\"]"))
```


## Tinder

https://policies.tinder.com/terms/intl/en

```{r}
#| label: scrapetinder

tinder_words <- analyse_platform("tinder",
  primary_url = "https://policies.tinder.com/terms/intl/en",
  start_date = as.Date("2012-08-15"),
  css_tries = c("#content-main", "#pbody"))
```

## Spotify

https://www.spotify.com/legal/end-user-agreement/

```{r}
#| label: scrapespotify

spotify_words <- analyse_platform("spotify",
  primary_url = "https://www.spotify.com/legal/end-user-agreement/",
  start_date = as.Date("2012-08-15"),
  css_tries = c("#content-main", "#pbody"))
```