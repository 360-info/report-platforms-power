---
title: "Platforms and Power"
subtitle: "Visualising the changing length of user agreements"
author: "James Goldie, 360info"
date: "2022-05-09"
code-fold: true
theme: style/article.scss
---

This demo analyses the changing length of the user agreements that major tech companies use. We'll use the [Wayback Machine API](https://archive.org/help/wayback_api.php) to identify snapshots of platform APIs, extract the text at different points in time, and analyse their word length.


```{r}
library(tidyverse)
library(httr)
library(rvest)
library(stringr)
library(tidytext)
library(lubridate)
library(collateral)
library(themes360info)
library(here)
```

## Test case: Facebook

Facebook's Terms of Service are currently at <https://www.facebook.com/terms.php>. Let's see the most recent Wayback Machine snapshot:

```{r}
#| label: demolookup
paste0(
  "http://archive.org/wayback/available?url=",
  "https://www.facebook.com/terms.php") %>%
  GET() ->
fb_snapshot

content(fb_snapshot)
```

We can access the newest snapshot at <`r content(fb_snapshot)$archived_snapshots$closest$url`> using the `{rvest}` package. The text of the agreement (as well as buttons to jump to the various sections) is available in the `#content` node:

```{r}
#| label: scrapefn
#| code-fold: true
#| code-summary: "Show the scraping and extraction function"

#' @param url The url of the page to look up (eg.
#'   https://www.facebook.com/terms.php)
#' @param css The css selector to use to extract text
#' @param dt A Date object, or a string in YYMMDD format, to try and get the
#'   snapshot from. If ommitted, the latest is used.
#' @return A list containing:
#'   - dt: the actual date-time of the snapshot
#'   - url: the url of the snapshot
#'   - words: a tidy data frame of word tokens as from tidytext::unnest_tokens()
get_words <- function(url, css, dt = NULL) {

  # construct the url to lookup the snapshot (add timestamp if requested)
  lookup_url <- paste0("http://archive.org/wayback/available?url=", url)
  if (!is_empty(dt)) {
    if (class(dt) == "Date") {
      lookup_url <- paste0(lookup_url, "&timestamp=", format(dt, "%Y%M%d"))
    } else {
      lookup_url <- paste0(lookup_url, "&timestamp=", dt)
    }
  }

  response <- GET(lookup_url)

  # check for missing content
  if (response$status != 200L) {
    warning(paste("HTTP", response$status, "thrown"))
  }
  is_available <- content(response)$archived_snapshots$closest$available
  if (is_empty(is_available) || (!is_available)) {
    stop("No snapshot available")
  }

  # extract the actual snapshot time and url
  snapshot_url <- content(response)$archived_snapshots$closest$url
  snapshot_dt <-
    content(response)$archived_snapshots$closest$timestamp %>%
    ymd_hms()

  # extract the snapshot content
  scrape <- read_html(snapshot_url)

  # extract the text from the page and break it by paragraph
  scrape %>%
    html_element(css) %>%
    html_text2() %>%
    str_split(regex("\n+")) %>%
    pluck(1) ->
  scrape_text

  scrape_text %>%
    tibble(line = 1:length(.), text = .) %>%
    unnest_tokens(word, text) ->
  scrape_words

  # return the words and the 
  return(list(
    dt = snapshot_dt,
    lookup_url = lookup_url,
    snapshot_url = snapshot_url,
    words = scrape_words
  ))
}
```

```{r}
demo_words <- get_words("https://www.facebook.com/terms.php", "#content")
```

The terms (including the Meta transition preface and the buttons) contain `r nrow(demo_words)` words.

### Comparing to past snapshots

But we want to see what this was like in the past. The Wayback Machine has thousands of scrapes of this URL over the last 15 years, and I don't think we can look at all. But we could probably look at, say, one each month. The real question is whether the page structure changes significantly enough over time to throw our function off.

[This scrape from 2018](https://web.archive.org/web/20180404004922/https://www.facebook.com/terms.php) still has the content in `#content`, which is good. So does [this scrape from 2015](https://web.archive.org/web/20150312205022/https://www.facebook.com/terms.php). [This one in 2012](https://web.archive.org/web/20120214045718/https://www.facebook.com/terms.php) redirects to `/legal/terms` (but scans fine). Same ad [this one in 2008](https://web.archive.org/web/20080220201805/https://www.facebook.com/terms.php).

Sampling a few scrapes quickly, it looks like the page structure has been consistent all the way back to the first Wayback snapshots back in [late 2005](https://web.archive.org/web/20051228025855/https://www.facebook.com/terms.php). So we're looking pretty good here!

:::{.callout-note}
Most of these terms link out to "supplementary terms of use" that govern the use of optional features or related products. I'm not going to try to analyse these for now, but we might come back to them!
:::

```{r}
seq(as.Date("2005-12-15"), Sys.Date(), by = "month") %>%
  tibble(
    url = "https://www.facebook.com/terms.php",
    css = "#content",
    dt = .) %>%
  # test: first and last two rows
  # slice(c(1:2, (n() - 1):n())) %>%
  mutate(
    words = pmap_peacefully(., get_words),
    lookup_url = map_chr(words, c("result", "lookup_url")),
    snapshot_url = map_chr(words, c("result", "snapshot_url")),
    par_count = map_int(words, ~ max(.x$result$words$line)),
    word_count = map_int(words, ~ nrow(.x$result$words))) ->
fb_terms_primary
```

How does that look?

```{r}
fb_terms_primary %>%
{
  ggplot(.) +
    aes(x = dt, y = word_count) +
    geom_area(fill = alpha("#3b5998", 0.75), colour = "#3b5998") +
    theme_360() +
    theme(
      panel.grid.major.x = element_blank(),
      panel.grid.minor.x = element_blank()
    ) +
    labs(x = NULL, y = "Word count",
      title = toupper("Facebook terms of use"))
}
```

I'm interested in why the terms spiked so high in the late 2000s before dropping back down in 2010. My suspicion is that Facebook has broken its terms up into "supplementary" terms of use for parts of its products, like its [Facebook Pages, Groups and Events Policy](Facebook Pages, Groups and Events Policy). Most people would consider groups and events fairly core features of Facebook, but these terms are broken into a separate document.

### Getting supplementary terms

One approach to handling these terms is to modify our scraper to pull out links and scrape them for the same date too. We need to be careful to screen out duplicate links too, though, and if those terms in turn link to others, we'd need to screen out ones we'd already scraped to avoid circular definitions.

```{r}
#| label: scrapelinksfn
#| code-fold: true
#| code-summary: "Show the scraping and extraction function"

#' @param url The url of the page to look up (eg.
#'   https://www.facebook.com/terms.php)
#' @param dt A Date object, or a string in YYMMDD format, to try and get the
#'   snapshot from. If ommitted, the latest is used.
#' @return A list containing:
#'   - dt: the actual date-time of the snapshot
#'   - snapshot_url: the url of the snapshot
request_snapshot <- function(url, dt) {

  # construct the url to lookup the snapshot (add timestamp if requested)
  lookup_url <- paste0("http://archive.org/wayback/available?url=", url)
  if (!is_empty(dt)) {
    if (class(dt) == "Date") {
      lookup_url <- paste0(lookup_url, "&timestamp=", format(dt, "%Y%M%d"))
    } else {
      lookup_url <- paste0(lookup_url, "&timestamp=", dt)
    }
  }

  response <- GET(lookup_url)

  # check for missing content
  if (response$status != 200L) {
    warning(paste("HTTP", response$status, "thrown"))
  }
  is_available <- content(response)$archived_snapshots$closest$available
  if (is_empty(is_available) || (!is_available)) {
    stop("No snapshot available")
  }

  # extract the actual snapshot time and url
  snapshot_url <- content(response)$archived_snapshots$closest$url
  snapshot_dt <-
    content(response)$archived_snapshots$closest$timestamp %>%
    ymd_hms()

  return(list(
    snapshot_dt = snapshot_dt,
    snapshot_url = snapshot_url))
}
```

```{r}
#| label: getwordslinksfn
#| code-fold: true
#| code-summary: "Show the words and links scraping function"

#' @param url The url of the page snapshot to look up (eg.
#'   https://web.archive.org/web/20220223013629/
#'     https://www.facebook.com/terms.php)
#' @param css The css selector to use to extract text
#' @return A list containing:
#'   - dt: the actual date-time of the snapshot
#'   - url: the url of the snapshot
#'   - words: a tidy data frame of word tokens as from tidytext::unnest_tokens
get_words_and_links <- function(snapshot_url, css) {

  # extract the snapshot content
  scrape <- read_html(snapshot_url)

  # identify links
  scrape %>%
    html_element(css) %>%
    html_elements("a") %>%
    { tibble(url = html_attr(., "href"), label = html_text(.)) } %>%
    # <a> that contain "policy", "policies", "terms"... but not "printable")
    filter(str_detect(label,
      regex("terms|policy|policies|notice", ignore_case = TRUE))) %>%
    filter(str_detect(label, "printable", negate = TRUE)) %>%
    filter(str_detect(label, "plain text", negate = TRUE)) %>%
    distinct(url, .keep_all = TRUE) %>%
    mutate(label = str_trim(label)) ->
  scrape_links

  # extract the text from the page and break it by paragraph
  scrape %>%
    html_element(css) %>%
    html_text2() %>%
    str_split(regex("\n+")) %>%
    pluck(1) ->
  scrape_text

  scrape_text %>%
    tibble(line = 1:length(.), text = .) %>%
    unnest_tokens(word, text) ->
  scrape_words

  # return the words and the 
  return(list(
    words = scrape_words,
    links = scrape_links
  ))
}
```

Now we can re-run this scraper, then run it on the extracted links! Because Wayback Machine substitues links in a snapshot with the snapshotted versions of those documents (and then redirects to a nearby snapshot of that document), I've split the process up into two steps:

1. `request_snapshot`: getting a snapshot URL from Wayback Machine, and
2. `get_words_and_links`: then actually scraping the screenshot.

We can skip the first step for the secondary policies!

```{r}
seq(as.Date("2005-12-15"), Sys.Date(), by = "month") %>%
  tibble(
    url = "https://www.facebook.com/terms.php",
    css = "#content",
    dt = .) %>%
  # test: first and last two rows
  slice(c(1:2, (n() - 1):n())) %>%
  mutate(
    # do the intiial lookup
    lookup = map2_peacefully(url, dt, request_snapshot),
    prim_snapshot_dt = map_chr(lookup, c("result", "snapshot_dt")),
    prim_snapshot_url = map_chr(lookup, c("result", "snapshot_url")),
    # then scrape the snapshot
    prim_scrape = map2_peacefully(prim_snapshot_url, css, get_words_and_links),
    prim_words = map_chr(lookup, c("result", "words")),
    prim_links = map_chr(lookup, c("result", "links"))) ->
fb_terms_firststage
```

Now we need to do the secondary scan based on the links!